{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TASK 1 - IMPROVE CBOW MODEL\n\n## MODEL B. TRAINED SCALAR WEIGHT\n\n*by Lucía De Pineda & Aina Luis*","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"from types import SimpleNamespace\nfrom collections import Counter\nimport os\nimport re\nimport pathlib\nimport array\nimport pickle\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-07T22:13:41.183689Z","iopub.execute_input":"2023-03-07T22:13:41.184149Z","iopub.status.idle":"2023-03-07T22:13:42.366547Z","shell.execute_reply.started":"2023-03-07T22:13:41.184109Z","shell.execute_reply":"2023-03-07T22:13:42.365325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Definition of the parameters","metadata":{}},{"cell_type":"code","source":"DATASET_VERSION = 'ca-100'\nCOMPETITION_ROOT = '../input/vectors4'\nDATASET_ROOT = f'../input/text-preprocessing/data/{DATASET_VERSION}'\nWORKING_ROOT = f'data/{DATASET_VERSION}'\nDATASET_PREFIX = 'ca.wiki'","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:42.626493Z","iopub.execute_input":"2023-03-07T22:13:42.627193Z","iopub.status.idle":"2023-03-07T22:13:42.634341Z","shell.execute_reply.started":"2023-03-07T22:13:42.627146Z","shell.execute_reply":"2023-03-07T22:13:42.633321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = SimpleNamespace(\n    embedding_dim = 100,\n    window_size = 7,\n    batch_size = 1000,\n    epochs = 4,\n    preprocessed = f'{DATASET_ROOT}/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}/{DATASET_PREFIX}',\n    modelname = f'{WORKING_ROOT}/{DATASET_VERSION}.pt',\n    train = True\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:42.986326Z","iopub.execute_input":"2023-03-07T22:13:42.987386Z","iopub.status.idle":"2023-03-07T22:13:42.994238Z","shell.execute_reply.started":"2023-03-07T22:13:42.987329Z","shell.execute_reply":"2023-03-07T22:13:42.993213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Definition of the classes and auxiliary functions","metadata":{}},{"cell_type":"code","source":"class Vocabulary(object):\n    def __init__(self, pad_token='<pad>', unk_token='<unk>', eos_token='<eos>'):\n        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token\n        if pad_token is not None:\n            self.pad_index = self.add_token(pad_token)\n        if unk_token is not None:\n            self.unk_index = self.add_token(unk_token)\n        if eos_token is not None:\n            self.eos_index = self.add_token(eos_token)\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def get_index(self, token):\n        if isinstance(token, str):\n            return self.token2idx.get(token, self.unk_index)\n        else:\n            return [self.token2idx.get(t, self.unk_index) for t in token]\n\n    def get_token(self, index):\n        return self.idx2token[index]\n\n    def __len__(self):\n        return len(self.idx2token)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, filename):\n        with open(filename, 'rb') as f:\n            self.__dict__.update(pickle.load(f))","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:43.457643Z","iopub.execute_input":"2023-03-07T22:13:43.458147Z","iopub.status.idle":"2023-03-07T22:13:43.475542Z","shell.execute_reply.started":"2023-03-07T22:13:43.458104Z","shell.execute_reply":"2023-03-07T22:13:43.473720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_generator(idata, target, batch_size, shuffle=True):\n    nsamples = len(idata)\n    if shuffle:\n        perm = np.random.permutation(nsamples)\n    else:\n        perm = range(nsamples)\n\n    for i in range(0, nsamples, batch_size):\n        batch_idx = perm[i:i+batch_size]\n        if target is not None:\n            yield idata[batch_idx], target[batch_idx]\n        else:\n            yield idata[batch_idx], None","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:43.803714Z","iopub.execute_input":"2023-03-07T22:13:43.805284Z","iopub.status.idle":"2023-03-07T22:13:43.813579Z","shell.execute_reply.started":"2023-03-07T22:13:43.805229Z","shell.execute_reply":"2023-03-07T22:13:43.812136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_preprocessed_dataset(prefix):\n    # Try loading precomputed vocabulary and preprocessed data files\n    token_vocab = Vocabulary()\n    token_vocab.load(f'{prefix}.vocab')\n    data = []\n    for part in ['train', 'valid', 'test']:\n        with np.load(f'{prefix}.{part}.npz') as set_data:\n            idata, target = set_data['idata'], set_data['target']\n            data.append((idata, target))\n            print(f'Number of samples ({part}): {len(target)}')\n    print(\"Using precomputed vocabulary and data files\")\n    print(f'Vocabulary size: {len(token_vocab)}')\n    return token_vocab, data","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:44.399164Z","iopub.execute_input":"2023-03-07T22:13:44.400616Z","iopub.status.idle":"2023-03-07T22:13:44.407651Z","shell.execute_reply.started":"2023-03-07T22:13:44.400557Z","shell.execute_reply":"2023-03-07T22:13:44.406348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer, idata, target, batch_size, device, log=False):\n    model.train()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    for X, y in batch_generator(idata, target, batch_size, shuffle=True):\n        # Get input and target sequences from batch\n        X = torch.tensor(X, dtype=torch.long, device=device)\n        y = torch.tensor(y, dtype=torch.long, device=device)\n\n        model.zero_grad()\n        output = model(X)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n        # Training statistics\n        total_loss += loss.item()\n        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n        ntokens += y.numel()\n        niterations += 1\n        if niterations == 200 or niterations == 500 or niterations % 1000 == 0:\n            print(f'Train: wpb={ntokens//niterations}, num_updates={niterations}, accuracy={100*ncorrect/ntokens:.1f}, loss={total_loss/ntokens:.2f}')\n\n    total_loss = total_loss / ntokens\n    accuracy = 100 * ncorrect / ntokens\n    if log:\n        print(f'Train: wpb={ntokens//niterations}, num_updates={niterations}, accuracy={accuracy:.1f}, loss={total_loss:.2f}')\n    return accuracy, total_loss","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:44.805061Z","iopub.execute_input":"2023-03-07T22:13:44.805492Z","iopub.status.idle":"2023-03-07T22:13:44.818051Z","shell.execute_reply.started":"2023-03-07T22:13:44.805452Z","shell.execute_reply":"2023-03-07T22:13:44.816838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, criterion, idata, target, batch_size, device):\n    model.eval()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    y_pred = []\n    with torch.no_grad():\n        for X, y in batch_generator(idata, target, batch_size, shuffle=False):\n            # Get input and target sequences from batch\n            X = torch.tensor(X, dtype=torch.long, device=device)\n            output = model(X)\n            if target is not None:\n                y = torch.tensor(y, dtype=torch.long, device=device)\n                loss = criterion(output, y)\n                total_loss += loss.item()\n                ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n                ntokens += y.numel()\n                niterations += 1\n            else:\n                pred = torch.max(output, 1)[1].detach().to('cpu').numpy()\n                y_pred.append(pred)\n\n    if target is not None:\n        total_loss = total_loss / ntokens\n        accuracy = 100 * ncorrect / ntokens\n        return accuracy, total_loss\n    else:\n        return np.concatenate(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:45.226762Z","iopub.execute_input":"2023-03-07T22:13:45.227176Z","iopub.status.idle":"2023-03-07T22:13:45.238780Z","shell.execute_reply.started":"2023-03-07T22:13:45.227139Z","shell.execute_reply":"2023-03-07T22:13:45.237255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting the working space and loading the dataset","metadata":{}},{"cell_type":"code","source":"# Create working dir\npathlib.Path(WORKING_ROOT).mkdir(parents=True, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:45.853585Z","iopub.execute_input":"2023-03-07T22:13:45.853995Z","iopub.status.idle":"2023-03-07T22:13:45.860325Z","shell.execute_reply.started":"2023-03-07T22:13:45.853960Z","shell.execute_reply":"2023-03-07T22:13:45.859128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select device\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    print(\"WARNING: Training without GPU can be very slow!\")","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:46.439658Z","iopub.execute_input":"2023-03-07T22:13:46.441360Z","iopub.status.idle":"2023-03-07T22:13:46.450138Z","shell.execute_reply.started":"2023-03-07T22:13:46.441297Z","shell.execute_reply":"2023-03-07T22:13:46.448257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab, data = load_preprocessed_dataset(params.preprocessed)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:48.553397Z","iopub.execute_input":"2023-03-07T22:13:48.553879Z","iopub.status.idle":"2023-03-07T22:14:22.062190Z","shell.execute_reply.started":"2023-03-07T22:13:48.553822Z","shell.execute_reply":"2023-03-07T22:14:22.060870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'El Periodico' validation dataset\nvalid_x_df = pd.read_csv(f'{COMPETITION_ROOT}/x_valid.csv')\ntokens = valid_x_df.columns[1:]\nvalid_x = valid_x_df[tokens].apply(vocab.get_index).to_numpy(dtype='int32')\nvalid_y_df = pd.read_csv(f'{COMPETITION_ROOT}/y_valid.csv')\nvalid_y = valid_y_df['token'].apply(vocab.get_index).to_numpy(dtype='int32')","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:14:22.064516Z","iopub.execute_input":"2023-03-07T22:14:22.065113Z","iopub.status.idle":"2023-03-07T22:14:22.266314Z","shell.execute_reply.started":"2023-03-07T22:14:22.065060Z","shell.execute_reply":"2023-03-07T22:14:22.265028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'El Periodico' test dataset\nvalid_x_df = pd.read_csv(f'{COMPETITION_ROOT}/x_test.csv')\ntest_x = valid_x_df[tokens].apply(vocab.get_index).to_numpy(dtype='int32')","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:14:22.269905Z","iopub.execute_input":"2023-03-07T22:14:22.270333Z","iopub.status.idle":"2023-03-07T22:14:22.421284Z","shell.execute_reply.started":"2023-03-07T22:14:22.270296Z","shell.execute_reply":"2023-03-07T22:14:22.419943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Definition of the model","metadata":{}},{"cell_type":"code","source":"class CBOW_trained_scalar_weight(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n        self.lin = nn.Linear(embedding_dim, num_embeddings, bias=False)\n        self.position_weight = nn.Parameter(torch.rand(1,6,1))\n\n    # B = Batch size\n    # W = Number of context words (left + right)\n    # E = embedding_dim\n    # V = num_embeddings (number of words)\n    def forward(self, input):\n        # input shape is (B, W)\n        e = self.emb(input)\n        # e shape is (B, W, E)\n        w = e*self.position_weight\n        print(self.position_weight)\n        u = w.sum(dim=1)\n        # u shape is (B, E)\n        z = self.lin(u)\n        # z shape is (B, V)\n        return z","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:15:09.145741Z","iopub.execute_input":"2023-03-07T22:15:09.146226Z","iopub.status.idle":"2023-03-07T22:15:09.157463Z","shell.execute_reply.started":"2023-03-07T22:15:09.146186Z","shell.execute_reply":"2023-03-07T22:15:09.155888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(123456)\nmodel = CBOW_trained_scalar_weight(len(vocab), params.embedding_dim).to(device)\nprint(model)\nfor name, param in model.named_parameters():\n    print(f'{name:20} {param.numel()} {list(param.shape)}')\nprint(f'TOTAL                {sum(p.numel() for p in model.parameters())}')","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:15:10.190456Z","iopub.execute_input":"2023-03-07T22:15:10.191013Z","iopub.status.idle":"2023-03-07T22:15:10.421191Z","shell.execute_reply.started":"2023-03-07T22:15:10.190963Z","shell.execute_reply":"2023-03-07T22:15:10.419608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(reduction='sum')","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:15:11.839653Z","iopub.execute_input":"2023-03-07T22:15:11.840230Z","iopub.status.idle":"2023-03-07T22:15:11.846385Z","shell.execute_reply.started":"2023-03-07T22:15:11.840180Z","shell.execute_reply":"2023-03-07T22:15:11.844965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and evaluation of the model","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters())\n\ntrain_accuracy = []\nwiki_accuracy = []\nvalid_accuracy = []\nfor epoch in range(params.epochs):\n    print(epoch)\n    acc, loss = train(model, criterion, optimizer, data[0][0], data[0][1], params.batch_size, device, log=True)\n    train_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | train accuracy={acc:.1f}%, train loss={loss:.2f}')\n    acc, loss = validate(model, criterion, data[1][0], data[1][1], params.batch_size, device)\n    wiki_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%, valid loss={loss:.2f} (wikipedia)')\n    acc, loss = validate(model, criterion, valid_x, valid_y, params.batch_size, device)\n    valid_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%, valid loss={loss:.2f} (El Periódico)')\n\n# Save model\ntorch.save(model.state_dict(), params.modelname)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:15:13.095123Z","iopub.execute_input":"2023-03-07T22:15:13.095577Z","iopub.status.idle":"2023-03-07T22:15:38.995574Z","shell.execute_reply.started":"2023-03-07T22:15:13.095540Z","shell.execute_reply":"2023-03-07T22:15:38.993821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-03-07T22:13:09.706481Z","iopub.execute_input":"2023-03-07T22:13:09.707595Z","iopub.status.idle":"2023-03-07T22:13:09.803677Z","shell.execute_reply.started":"2023-03-07T22:13:09.707546Z","shell.execute_reply":"2023-03-07T22:13:09.801762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test prediction\ny_pred = validate(model, None, test_x, None, params.batch_size, device)\ny_token = [vocab.idx2token[index] for index in y_pred]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':valid_x_df['id'], 'token': y_token}, columns=['id', 'token'])\nprint(submission.head())\nsubmission.to_csv('submissionB.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}